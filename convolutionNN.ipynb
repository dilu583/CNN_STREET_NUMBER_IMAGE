{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# These are all the modules I'll be using later. \n",
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import tarfile\n",
    "from IPython.display import display, Image\n",
    "from scipy import ndimage\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from six.moves import cPickle as pickle\n",
    "\n",
    "import pandas as pd\n",
    "import urllib\n",
    "import cPickle as pickle\n",
    "from PIL import Image\n",
    "import scipy.io as sio\n",
    "import scipy.io\n",
    "import h5py\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import tensorboard as tb\n",
    "from sklearn.cross_validation import train_test_split as ttsplit\n",
    "import math as math\n",
    "import pylab as P\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (230540, 32, 32, 1) (230540, 6)\n",
      "Validation set (5214, 32, 32, 1) (5214, 6)\n",
      "Test set (13068, 32, 32, 1) (13068, 6)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'SVHN.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = train_labels[:64]\n",
    "t = test[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "patch_size = 5\n",
    "image_size = 32\n",
    "label_size = 6\n",
    "num_channels = 1 # gray scale\n",
    "num_labels = 11\n",
    "depth1 = 16\n",
    "depth2 = 32\n",
    "num_hidden = 64\n",
    "keep_prob = 0.98"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 2).T == labels)) / labels.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    tf_train_dataset  = tf.placeholder(tf.float32, shape=[batch_size,image_size,image_size,num_channels])\n",
    "    tf_train_labels = tf.placeholder(tf.int32, shape=[batch_size,label_size])\n",
    "    \n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "    #variables.\n",
    "    layer1_weights = tf.Variable(tf.truncated_normal([patch_size,patch_size,num_channels,depth1],stddev=0.1))\n",
    "    layer1_biases = tf.Variable(tf.zeros([depth1]))\n",
    "    \n",
    "    layer2_weights = tf.Variable(tf.truncated_normal([patch_size,patch_size,depth1,depth2],stddev=0.1))\n",
    "    layer2_biases = tf.Variable(tf.constant(1.0,shape=[depth2]))\n",
    "    #convolution layer that connect after convolution to fully connected output layer\n",
    "    layer3_weights = tf.Variable(tf.truncated_normal([image_size // 4 * image_size // 4 * depth2, num_hidden], stddev=0.1))\n",
    "    layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "    \n",
    "    #output\n",
    "    op1_weights = tf.Variable(tf.truncated_normal([num_hidden,num_labels],stddev=0.1))\n",
    "    op1_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "    \n",
    "    op2_weights = tf.Variable(tf.truncated_normal([num_hidden,num_labels],stddev=0.1))\n",
    "    op2_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "    \n",
    "    op3_weights = tf.Variable(tf.truncated_normal([num_hidden,num_labels],stddev=0.1))\n",
    "    op3_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "    \n",
    "    op4_weights = tf.Variable(tf.truncated_normal([num_hidden,num_labels],stddev=0.1))\n",
    "    op4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "    \n",
    "    op5_weights = tf.Variable(tf.truncated_normal([num_hidden,num_labels],stddev=0.1))\n",
    "    op5_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "    \n",
    "    #construct model\n",
    "    def model(data, is_training=False):\n",
    "        conv = tf.nn.conv2d(data, layer1_weights, [1,1,1,1], padding='SAME')\n",
    "        hidden = tf.nn.relu(conv + layer1_biases)\n",
    "        nomalization = tf.nn.local_response_normalization(hidden)\n",
    "        pool = tf.nn.max_pool(nomalization,[1,2,2,1],[1,2,2,1],padding = 'SAME')\n",
    "        \n",
    "        conv = tf.nn.conv2d(pool, layer2_weights, [1,1,1,1], padding='SAME')\n",
    "        hidden = tf.nn.relu(conv + layer2_biases)\n",
    "        nomalization = tf.nn.local_response_normalization(hidden)\n",
    "        pool = tf.nn.max_pool(nomalization,[1,2,2,1],[1,2,2,1],padding = 'SAME')\n",
    "        #final layer before connecting to the output layer\n",
    "        \n",
    "        hidden_dropout = tf.nn.dropout(pool, keep_prob)\n",
    "        shape = hidden_dropout.get_shape().as_list()\n",
    "        reshaped = tf.reshape(hidden_dropout,[shape[0],shape[1]*shape[2]*shape[3]])\n",
    "        \n",
    "        reshaped = tf.nn.relu(tf.matmul(reshaped, layer3_weights) + layer3_biases)\n",
    "        \n",
    "        logits1 = tf.matmul(reshaped,op1_weights) + op1_biases\n",
    "        logits2 = tf.matmul(reshaped,op2_weights) + op2_biases\n",
    "        logits3 = tf.matmul(reshaped,op3_weights) + op3_biases\n",
    "        logits4 = tf.matmul(reshaped,op4_weights) + op4_biases\n",
    "        logits5 = tf.matmul(reshaped,op5_weights) + op5_biases\n",
    "\n",
    "        return (logits1,logits2,logits3,logits4,logits5)\n",
    "    logits1,logits2,logits3,logits4,logits5 = model(tf_train_dataset,True)\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.sparse_softmax_cross_entropy_with_logits(logits1, tf_train_labels[:,1])\n",
    "     +  tf.nn.sparse_softmax_cross_entropy_with_logits(logits2, tf_train_labels[:,2])\n",
    "     +  tf.nn.sparse_softmax_cross_entropy_with_logits(logits3, tf_train_labels[:,3])\n",
    "     +  tf.nn.sparse_softmax_cross_entropy_with_logits(logits4, tf_train_labels[:,4])\n",
    "     +  tf.nn.sparse_softmax_cross_entropy_with_logits(logits5, tf_train_labels[:,5])\n",
    "    )\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.15, global_step, 1000, 0.93)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "    train_prediction = tf.pack([tf.nn.softmax(logits1), tf.nn.softmax(logits2), tf.nn.softmax(logits3),\n",
    "                              tf.nn.softmax(logits4),tf.nn.softmax(logits5)])\n",
    "    valid_prediction = tf.pack([tf.nn.softmax(model(tf_valid_dataset)[0]), tf.nn.softmax(model(tf_valid_dataset)[1]),\n",
    "                              tf.nn.softmax(model(tf_valid_dataset)[2]), tf.nn.softmax(model(tf_valid_dataset)[3]),\n",
    "                              tf.nn.softmax(model(tf_valid_dataset)[4])])\n",
    "    test_prediction = tf.pack([tf.nn.softmax(model(tf_test_dataset)[0]), tf.nn.softmax(model(tf_test_dataset)[1]),\n",
    "                             tf.nn.softmax(model(tf_test_dataset)[2]), tf.nn.softmax(model(tf_test_dataset)[3]),\n",
    "                             tf.nn.softmax(model(tf_test_dataset)[4])])\n",
    "    saver = tf.train.Saver()\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bathch_la = train_labels[:64,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 14.762002\n",
      "Minibatch accuracy: 10.0%\n",
      "Validation accuracy: 60.4%\n",
      "Minibatch loss at step 500: 6.233685\n",
      "Minibatch accuracy: 56.9%\n",
      "Validation accuracy: 60.2%\n",
      "Minibatch loss at step 1000: 4.992568\n",
      "Minibatch accuracy: 68.1%\n",
      "Validation accuracy: 71.7%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 10001\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "  tf.initialize_all_variables().run()\n",
    "\n",
    "\n",
    "\n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size),:]\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = sess.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    \n",
    "    if (step % 500 == 0): \n",
    "      print('Minibatch loss at step %d: %f' % (step, l))\n",
    "      print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels[:,1:6]))\n",
    "      print('Validation accuracy: %.1f%%' % accuracy(valid_prediction.eval(), valid_labels[:,1:6]))\n",
    "  final_predictions = test_prediction.eval()\n",
    "  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels[:,1:6]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
